{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "np.random.seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, number_of_states, number_of_actions):\n",
    "    policy = np.zeros((1, number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    old_value_list = value_list.copy()\n",
    "    episode = 0\n",
    "    max_change = 1\n",
    "    sigma = 0.9\n",
    "    while max_change > 0.01:\n",
    "        episode += 1\n",
    "        for s in range(number_of_states):\n",
    "            assigned_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get new state and its reward        \n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    # get new states value\n",
    "                    value_new_state = old_value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward \n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += cand_value*prob \n",
    "                        \n",
    "                if total_cand_value > assigned_value:\n",
    "                    assigned_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "                    value_list[0][s] = assigned_value\n",
    "        changes = np.abs(value_list - old_value_list)\n",
    "        max_change = np.max(changes)\n",
    "        old_value_list = value_list.copy()\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "    return value_list, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, number_of_states, number_of_actions):\n",
    "    \n",
    "    ## 1\n",
    "    policy = np.random.randint(number_of_actions, size=(1,number_of_states))\n",
    "#     policy = np.zeros((1,number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    episode = 0\n",
    "    sigma = 0.9\n",
    "    \n",
    "    ## 2\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        episode += 1\n",
    "        eval_acc = True\n",
    "        while eval_acc:\n",
    "            eps = 0\n",
    "            for s in range(number_of_states):\n",
    "                # first row\n",
    "                v = value_list[0][s]\n",
    "\n",
    "                # get the new value \n",
    "                a = policy[0][s]\n",
    "                total_val_new_state = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    # second row\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward\n",
    "                        # value_list[0][s] = reward\n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_val_new_state += cand_value*prob \n",
    "                value_list[0][s] = total_val_new_state\n",
    "                    \n",
    "                # third row\n",
    "                eps = max(eps, np.abs(v-value_list[0][s]))\n",
    "            if eps < 0.001:\n",
    "                eval_acc = False\n",
    "\n",
    "\n",
    "        ## 3\n",
    "        policy_stable = True\n",
    "        for s in range(number_of_states):\n",
    "\n",
    "            # assign \n",
    "            old_action = policy[0][s]\n",
    "            # get the argmax a here\n",
    "            max_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get the new value \n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward\n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += prob*cand_value\n",
    "                if total_cand_value > max_value:\n",
    "                    max_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "\n",
    "            # if old-action != policy[s]\n",
    "            if old_action != policy[0][s]:\n",
    "                policy_stable = False\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "\n",
    "    return value_list, policy       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"FrozenLake-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  4  episodes\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "current_state = env.reset()\n",
    "value_list, policy = policy_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 2., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  10  episodes\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "current_state = env.reset()\n",
    "value_list, policy = value_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True 0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "act = int(policy[0][current_state])\n",
    "\n",
    "print(act)\n",
    "new_state, reward, finished, _ = env.step(act)\n",
    "rewards.append(reward)\n",
    "current_state = new_state\n",
    "print(finished, reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
