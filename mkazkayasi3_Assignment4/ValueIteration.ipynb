{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "np.random.seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, number_of_states, number_of_actions):\n",
    "    policy = np.zeros((1, number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    old_value_list = value_list.copy()\n",
    "    episode = 0\n",
    "    max_change = 1\n",
    "    sigma = 0.9\n",
    "    while max_change > 1e-9:\n",
    "        episode += 1\n",
    "        for s in range(number_of_states):\n",
    "            assigned_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get new state and its reward        \n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    # get new states value\n",
    "                    value_new_state = old_value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward \n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += cand_value*prob \n",
    "                        \n",
    "                if total_cand_value > assigned_value:\n",
    "                    assigned_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "                    value_list[0][s] = assigned_value\n",
    "        changes = np.abs(value_list - old_value_list)\n",
    "        max_change = np.max(changes)\n",
    "        old_value_list = value_list.copy()\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "    return value_list, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, number_of_states, number_of_actions):\n",
    "    \n",
    "    ## 1\n",
    "    policy = np.random.randint(number_of_actions, size=(1,number_of_states))\n",
    "#     policy = np.zeros((1,number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    episode = 0\n",
    "    sigma = 0.9\n",
    "    \n",
    "    ## 2\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        episode += 1\n",
    "        eval_acc = True\n",
    "        while eval_acc:\n",
    "            eps = 0\n",
    "            for s in range(number_of_states):\n",
    "                # first row\n",
    "                v = value_list[0][s]\n",
    "\n",
    "                # get the new value \n",
    "                a = policy[0][s]\n",
    "                total_val_new_state = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    # second row\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward\n",
    "                        # value_list[0][s] = reward\n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_val_new_state += cand_value*prob \n",
    "                value_list[0][s] = total_val_new_state\n",
    "                    \n",
    "                # third row\n",
    "                eps = max(eps, np.abs(v-value_list[0][s]))\n",
    "            if eps < 1e-9:\n",
    "                eval_acc = False\n",
    "\n",
    "\n",
    "        ## 3\n",
    "        policy_stable = True\n",
    "        for s in range(number_of_states):\n",
    "\n",
    "            # assign \n",
    "            old_action = policy[0][s]\n",
    "            # get the argmax a here\n",
    "            max_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get the new value \n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward\n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += prob*cand_value\n",
    "                if total_cand_value > max_value:\n",
    "                    max_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "\n",
    "            # if old-action != policy[s]\n",
    "            if old_action != policy[0][s]:\n",
    "                policy_stable = False\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "\n",
    "    return value_list, policy       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-9 < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"FrozenLake-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False)]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  9  episodes\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', map_name=\"8x8\").env\n",
    "current_state = env.reset()\n",
    "value_list, policy = policy_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  141  episodes\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', map_name=\"8x8\").env\n",
    "current_state = env.reset()\n",
    "value_list, policy = value_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "False 0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "act = int(policy[0][current_state])\n",
    "\n",
    "print(act)\n",
    "new_state, reward, finished, _ = env.step(act)\n",
    "rewards.append(reward)\n",
    "current_state = new_state\n",
    "print(finished, reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 0.0 0\n",
      "False 0.0 0\n",
      "False 0.0 0\n",
      "False 0.0 1\n",
      "False 0.0 1\n",
      "False 0.0 1\n",
      "False 0.0 1\n",
      "False 0.0 1\n",
      "False 0.0 9\n",
      "False 0.0 10\n",
      "False 0.0 11\n",
      "False 0.0 3\n",
      "False 0.0 11\n",
      "False 0.0 3\n",
      "False 0.0 11\n",
      "False 0.0 12\n",
      "False 0.0 13\n",
      "False 0.0 21\n",
      "False 0.0 20\n",
      "False 0.0 28\n",
      "False 0.0 36\n",
      "False 0.0 44\n",
      "False 0.0 36\n",
      "False 0.0 44\n",
      "False 0.0 45\n",
      "False 0.0 37\n",
      "False 0.0 38\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 37\n",
      "False 0.0 45\n",
      "False 0.0 44\n",
      "False 0.0 43\n",
      "False 0.0 44\n",
      "False 0.0 36\n",
      "False 0.0 37\n",
      "False 0.0 38\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 22\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 22\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 37\n",
      "False 0.0 45\n",
      "False 0.0 37\n",
      "False 0.0 45\n",
      "False 0.0 53\n",
      "False 0.0 45\n",
      "False 0.0 37\n",
      "False 0.0 45\n",
      "False 0.0 37\n",
      "False 0.0 38\n",
      "False 0.0 37\n",
      "False 0.0 36\n",
      "False 0.0 28\n",
      "False 0.0 20\n",
      "False 0.0 12\n",
      "False 0.0 20\n",
      "False 0.0 21\n",
      "False 0.0 22\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 22\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 47\n",
      "False 0.0 47\n",
      "False 0.0 55\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 47\n",
      "False 0.0 39\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 37\n",
      "False 0.0 45\n",
      "False 0.0 37\n",
      "False 0.0 36\n",
      "False 0.0 44\n",
      "False 0.0 45\n",
      "False 0.0 44\n",
      "False 0.0 43\n",
      "False 0.0 51\n",
      "True 0.0 59\n",
      "Finished at episode:  159\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFF\u001b[41mH\u001b[0mFFFG\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "ep = 0\n",
    "while not done:\n",
    "    ep += 1\n",
    "    act = int(policy[0][current_state])\n",
    "    new_state, reward, done, _ = env.step(act)\n",
    "    current_state = new_state\n",
    "    print(done, reward, new_state)\n",
    "print(\"Finished at episode: \", ep)\n",
    "env.render()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
