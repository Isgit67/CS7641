{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, number_of_states, number_of_actions):\n",
    "    policy = np.zeros((1, number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    old_value_list = value_list.copy()\n",
    "    episode = 0\n",
    "    max_change = 1\n",
    "    sigma = 0.9\n",
    "    while max_change > 0.01:\n",
    "        episode += 1\n",
    "        for s in range(number_of_states):\n",
    "            assigned_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get new state and its reward\n",
    "                prob, new_state, reward, done = env.P[s][a][0]\n",
    "                # get new states value\n",
    "                value_new_state = old_value_list[0][new_state]\n",
    "                cand_value = 0\n",
    "                if done:\n",
    "                    cand_value = reward \n",
    "                else:\n",
    "                    cand_value = reward + sigma*value_new_state\n",
    "                if cand_value > assigned_value:\n",
    "                    assigned_value = cand_value\n",
    "                    policy[0][s] = a\n",
    "                    value_list[0][s] = assigned_value\n",
    "        changes = np.abs(value_list - old_value_list)\n",
    "        max_change = np.max(changes)\n",
    "        old_value_list = value_list.copy()\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "    return value_list, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, number_of_states, number_of_actions):\n",
    "    \n",
    "    ## 1\n",
    "#     policy = np.random.randint(6, size=(1,number_of_states))\n",
    "    policy = np.zeros((1,number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    episode = 0\n",
    "    sigma = 0.9\n",
    "    \n",
    "    ## 2\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        episode += 1\n",
    "        eval_acc = True\n",
    "        while eval_acc:\n",
    "            eps = 0\n",
    "            for s in range(number_of_states):\n",
    "                # first row\n",
    "                v = value_list[0][s]\n",
    "\n",
    "                # get the new value \n",
    "                a = policy[0][s]\n",
    "                prob, new_state, reward, done = env.P[s][a][0]\n",
    "                value_new_state = value_list[0][new_state]\n",
    "\n",
    "                # second row\n",
    "                if done:\n",
    "                    value_list[0][s] = reward\n",
    "                else:\n",
    "                    value_list[0][s] = reward + sigma*value_new_state\n",
    "\n",
    "                # third row\n",
    "                eps = max(eps, np.abs(v-value_list[0][s]))\n",
    "            if eps < 0.001:\n",
    "                eval_acc = False\n",
    "\n",
    "\n",
    "        ## 3\n",
    "        policy_stable = True\n",
    "        for s in range(number_of_states):\n",
    "\n",
    "            # assign \n",
    "            old_action = policy[0][s]\n",
    "\n",
    "            # get the argmax a here\n",
    "            max_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get the new value \n",
    "                prob, new_state, reward, done = env.P[s][a][0]\n",
    "                value_new_state = value_list[0][new_state]\n",
    "                cand_value = 0\n",
    "                if done:\n",
    "                    cand_value = reward\n",
    "                else:\n",
    "                    cand_value = reward + sigma*value_new_state\n",
    "\n",
    "                if cand_value > max_value:\n",
    "                    max_value = cand_value\n",
    "                    policy[0][s] = a\n",
    "\n",
    "            # if old-action != policy[s]\n",
    "            if old_action != policy[0][s]:\n",
    "                policy_stable = False\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "\n",
    "    return value_list, policy       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  17  episodes\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "current_state = env.reset()\n",
    "value_list, policy = policy_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  19  episodes\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "current_state = env.reset()\n",
    "value_list, policy = value_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "True 20\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "act = int(policy[0][current_state])\n",
    "\n",
    "print(act)\n",
    "new_state, reward, finished, _ = env.step(act)\n",
    "rewards.append(reward)\n",
    "current_state = new_state\n",
    "print(finished, reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
