{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "np.random.seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, number_of_states, number_of_actions):\n",
    "    policy = np.zeros((1, number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    old_value_list = value_list.copy()\n",
    "    episode = 0\n",
    "    max_change = 1\n",
    "    sigma = 0.9\n",
    "    while max_change > 1e-12:\n",
    "        episode += 1\n",
    "        for s in range(number_of_states):\n",
    "            assigned_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get new state and its reward        \n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    # get new states value\n",
    "                    value_new_state = old_value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward \n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += cand_value*prob \n",
    "                        \n",
    "                if total_cand_value > assigned_value:\n",
    "                    assigned_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "                    value_list[0][s] = assigned_value\n",
    "        changes = np.abs(value_list - old_value_list)\n",
    "        max_change = np.max(changes)\n",
    "        old_value_list = value_list.copy()\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "    return value_list, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, number_of_states, number_of_actions):\n",
    "    \n",
    "    ## 1\n",
    "    policy = np.random.randint(number_of_actions, size=(1,number_of_states))\n",
    "#     policy = np.zeros((1,number_of_states))\n",
    "    value_list = np.zeros((1, number_of_states))\n",
    "    episode = 0\n",
    "    sigma = 0.9\n",
    "    \n",
    "    ## 2\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        episode += 1\n",
    "        eval_acc = True\n",
    "        while eval_acc:\n",
    "            eps = 0\n",
    "            for s in range(number_of_states):\n",
    "                # first row\n",
    "                v = value_list[0][s]\n",
    "\n",
    "                # get the new value \n",
    "                a = policy[0][s]\n",
    "                total_val_new_state = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    # second row\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward\n",
    "                        # value_list[0][s] = reward\n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_val_new_state += cand_value*prob \n",
    "                value_list[0][s] = total_val_new_state\n",
    "                    \n",
    "                # third row\n",
    "                eps = max(eps, np.abs(v-value_list[0][s]))\n",
    "            if eps < 1e-12:\n",
    "                eval_acc = False\n",
    "\n",
    "\n",
    "        ## 3\n",
    "        policy_stable = True\n",
    "        for s in range(number_of_states):\n",
    "\n",
    "            # assign \n",
    "            old_action = policy[0][s]\n",
    "            # get the argmax a here\n",
    "            max_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                # get the new value \n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[0][new_state]\n",
    "                    cand_value = 0\n",
    "                    if done:\n",
    "                        cand_value = reward\n",
    "                    else:\n",
    "                        cand_value = reward + sigma*value_new_state\n",
    "                    total_cand_value += prob*cand_value\n",
    "                if total_cand_value > max_value:\n",
    "                    max_value = total_cand_value\n",
    "                    policy[0][s] = a\n",
    "\n",
    "            # if old-action != policy[s]\n",
    "            if old_action != policy[0][s]:\n",
    "                policy_stable = False\n",
    "    print(\"Solved in: \", episode, \" episodes\")\n",
    "\n",
    "    return value_list, policy       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  17  episodes\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('FrozenLake-v0', map_name=\"8x8\").env\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "current_state = env.reset()\n",
    "value_list, policy = policy_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in:  193  episodes\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', map_name=\"8x8\").env\n",
    "# env = gym.make(\"Taxi-v3\")\n",
    "# env = gym.make(\"HotterColder-v0\")\n",
    "current_state = env.reset()\n",
    "value_list, policy = value_iteration(env, env.observation_space.n, env.action_space.n)\n",
    "rewards = []\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "False 0.0\n",
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "act = int(policy[0][current_state])\n",
    "\n",
    "print(act)\n",
    "new_state, reward, finished, _ = env.step(act)\n",
    "rewards.append(reward)\n",
    "current_state = new_state\n",
    "print(finished, reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 0.0 15\n",
      "False 0.0 14\n",
      "False 0.0 22\n",
      "False 0.0 14\n",
      "False 0.0 22\n",
      "False 0.0 14\n",
      "False 0.0 22\n",
      "False 0.0 14\n",
      "False 0.0 6\n",
      "False 0.0 6\n",
      "False 0.0 7\n",
      "False 0.0 7\n",
      "False 0.0 7\n",
      "False 0.0 7\n",
      "False 0.0 15\n",
      "False 0.0 23\n",
      "False 0.0 22\n",
      "False 0.0 23\n",
      "False 0.0 23\n",
      "False 0.0 31\n",
      "False 0.0 39\n",
      "False 0.0 31\n",
      "False 0.0 31\n",
      "False 0.0 31\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 37\n",
      "False 0.0 38\n",
      "False 0.0 30\n",
      "False 0.0 38\n",
      "False 0.0 37\n",
      "False 0.0 38\n",
      "False 0.0 37\n",
      "False 0.0 36\n",
      "False 0.0 37\n",
      "False 0.0 36\n",
      "False 0.0 28\n",
      "False 0.0 20\n",
      "False 0.0 28\n",
      "False 0.0 36\n",
      "False 0.0 44\n",
      "False 0.0 45\n",
      "False 0.0 53\n",
      "False 0.0 45\n",
      "False 0.0 44\n",
      "False 0.0 43\n",
      "False 0.0 51\n",
      "False 0.0 43\n",
      "False 0.0 44\n",
      "False 0.0 45\n",
      "False 0.0 44\n",
      "False 0.0 45\n",
      "False 0.0 53\n",
      "True 0.0 52\n",
      "Finished at step:  54\n",
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFF\u001b[41mH\u001b[0mFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "ep = 0\n",
    "while not done:\n",
    "    ep += 1\n",
    "    act = int(policy[0][current_state])\n",
    "    new_state, reward, done, _ = env.step(act)\n",
    "    current_state = new_state\n",
    "    print(done, reward, new_state)\n",
    "print(\"Finished at step: \", ep)\n",
    "env.render()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
